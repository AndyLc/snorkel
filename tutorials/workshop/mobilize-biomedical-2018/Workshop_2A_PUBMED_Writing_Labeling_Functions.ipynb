{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE\n",
    "\n",
    "Please make certain that the `DBNAME` variable in `lib/init.py` is set to\n",
    "\n",
    "    DBNAME = 'CDR'\n",
    "    \n",
    "to use this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snorkel Workshop: Extracting `Chemical-Disease` Relations (CDR) from PubMed Abstracts\n",
    "## Part 2: Writing  Labeling Functions\n",
    "\n",
    "In Snorkel, our primary interface through which we provide training signal to the end extraction model we are training is by writing **labeling functions (LFs)** (as opposed to hand-labeling massive training sets).  We'll go through some examples for our `Chemical-Disease` extraction task below.\n",
    "\n",
    "A labeling function isn't anything special. It's just a Python function that accepts a `Candidate` as the input argument and returns `1` if it says the `Candidate` should be marked as true, `-1` if it says the `Candidate` should be marked as false, and `0` if it doesn't know how to vote and abstains. In practice, many labeling functions are unipolar: it labels only `1`s and `0`s, or it labels only `-1`s and `0`s.\n",
    "\n",
    "Recall that our goal is to ultimately train a high-performance classification model that predicts which of our `Candidate`s are true mentions of chemical-disease relations.  It turns out that we can do this by writing potentially low-quality labeling functions and aggregating them using Snorkel into higher quality labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Connect to the database backend and initalize a Snorkel session\n",
    "from lib.init import *\n",
    "from lib.scoring import *\n",
    "from lib.lf_factories import *\n",
    "\n",
    "from snorkel.lf_helpers import test_LF\n",
    "from snorkel.annotations import load_gold_labels\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "# initialize our candidate type definition\n",
    "ChemicalDisease = candidate_subclass('ChemicalDisease', ['chemical', 'disease'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Background\n",
    "\n",
    "## A. Task Definition\n",
    "\n",
    "\n",
    "The CDR task is comprised of three sets of 500 documents each, called training, development, and test. A document consists of the title and abstract of an article from [PubMed](https://www.ncbi.nlm.nih.gov/pubmed/), an archive of biomedical and life sciences journal literature. The documents have been hand-annotated with\n",
    "\n",
    "* Mentions of chemicals and diseases along with their [MESH](https://meshb.nlm.nih.gov/#/fieldSearch) IDs, canonical IDs for medical entities. For example, mentions of \"warfarin\" in two different documents will have the same ID.\n",
    "* Chemical-disease relations at the document-level. That is, if some piece of text in the document implies that a chemical with MESH ID `X` induces a disease with MESH ID `Y`, the document will be annotated with `Relation(X, Y)`\n",
    "\n",
    "We'll be writing an application to extract mentions of `chemical-induced-disease` relationships from Pubmed abstracts, as per the BioCreative CDR Challenge. \n",
    "\n",
    "\n",
    "## B. Preprocessing the Database\n",
    "\n",
    "In a real application, there is a lot of data preparation, parsing, and database loading that needs to be completed before we dive into writing labeling functions. Here we've pre-generated a database instance for you. All _candidates_ and _gold labels_ (i.e., human-generated labels) are queried from this database for use in the the tutorial. \n",
    "\n",
    "Download and launch our preprocessing tutorial <a href=\"https://github.com/HazyResearch/snorkel/tree/master/tutorials/workshop\">Workshop 5 Advanced Preprocessing</a> from our GitHub page for more details on how this database is built.\n",
    "\n",
    "## C. Using a _Development Set_ of Human-labeled Data\n",
    "\n",
    "In our setting, we will use the phrase _development set_ to refer to a set of examples (here, a subset of our training set) which we label by hand and use to help us develop and refine labeling functions.  Unlike the _test set_, which we do not look at and use for final evaluation, we can inspect the development set while writing labeling functions. This is a list of `{-1,1}` labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Data Exploration\n",
    "\n",
    "### 1. Viewing Sentences\n",
    "How do we come up with good keywords and patterns to encode as labeling functions? One way is to manually explore our training data. Here we load a subset of our training candidates into a `SentenceNgramViewer` object to examine candidates in their parent context. Our goal is to build an intuition for patterns and keywords that are predictive of a candidate's true label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.viewer import SentenceNgramViewer\n",
    "\n",
    "# load our list of training & development candidates\n",
    "train_cands = session.query(Candidate).filter(Candidate.split == 0).order_by(Candidate.id).all()\n",
    "dev_cands   = session.query(Candidate).filter(Candidate.split == 1).order_by(Candidate.id).all()\n",
    "\n",
    "SentenceNgramViewer(train_cands[0:500], session, n_per_page=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploring Gold Labeled Data\n",
    "\n",
    "We can use another visualization tool to explore a small set of labeled candidates to help build out inutution for posiive and negative class examples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib.viz import *\n",
    "\n",
    "POSITIVE, UNLABLED, NEGATIVE = 1, 0, -1\n",
    "view_labeled_candidates(dev_cands, L_gold_dev, label=POSITIVE, n_max=5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Labeling Function Metrics\n",
    "\n",
    "### 1. Coverage\n",
    "One simple metric we can compute quickly is our _coverage_, the number of candidates labeled by our LF $\\lambda_j$, on our training set (or any other set).\n",
    "\n",
    "\\begin{eqnarray}\n",
    "coverage_{\\lambda_j} = \\frac{1}{N}\\sum_{i=1}^{N} \\unicode{x1D7D9}({\\lambda_j}{(x_i) \\in \\{-1,1\\}})\n",
    "\\label{eq3}\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "### 2. Precision / Recall / F1\n",
    "If we have gold labeled data, we can also compute standard precision, recall, and F1 metrics for the output of a single labeling function. These metrics are computed over 4 _error buckets_: _True Positives_ (tp), _False Positives_ (fp), _True Negatives_ (tn), and _False Negatives_ (fn).\n",
    "\n",
    "\\begin{equation*}\n",
    "precision = \\frac{tp}{(tp + fp)}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "recall = \\frac{tp}{(tp + fn)}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "F1 = 2 \\cdot \\frac{ (precision \\cdot recall)}{(precision + recall)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Labeling Functions\n",
    "\n",
    "## A. Pattern Matching Labeling Functions\n",
    "\n",
    "One powerful form of labeling function design is defining sets of keywords or regular expressions that, as a human labeler, you know are correlated with the true label. In the terminology of [Bayesian inference](https://en.wikipedia.org/wiki/Statistical_inference#Bayesian_inference), this can be thought of as defining a [_prior_](https://en.wikipedia.org/wiki/Prior_probability) over your word features. \n",
    "\n",
    "For example, we could define a dictionary of terms that occur between chemical and disease names in a candidate. One simple dictionary of terms indicating a true relation could be:\n",
    "    \n",
    "    treatment = {'treatment'}\n",
    " \n",
    "We can then write a labeling function that checks for a match with these terms in the text that occurs between person names.\n",
    "\n",
    "    def LF_treatment_terms_between(c):\n",
    "        return 1 if len(treat.intersection(get_between_tokens(c))) > 0 else 0\n",
    "        \n",
    "The idea is that we can easily create dictionaries that encode themes or categories descibing different types of interactions between a `chemical` and `disease` and then use these objects to _weakly supervise_ our classification task.\n",
    "\n",
    "**IMPORTANT** Good labeling functions manage a trade-off between high coverage and high precision. When constructing your dictionaries, think about building larger, noiser sets of terms instead of relying on 1 or 2 keywords. Sometimes a single word can be very predictive but it's almost always better to define something more general, such as a regular expressions or dictionaries of many related words. \n",
    "    \n",
    "\n",
    "### 1. Labeling Function Factories\n",
    "The above is a reasonable way to write labeling functions. However, this type of design pattern is so common that we rely on another abstraction to help us build LFs more quickly: _labeling function factories_. Factories accept simple inputs, like dictionaries or a set of regular expressions, and automatically builds labeling functions for you.\n",
    "\n",
    "The `MatchTerms` and `MatchRegex` factories require a few parameter definitions to setup:\n",
    "    \n",
    "    name:    a string that describes the category of terms/regular expressons\n",
    "    label:   patterns correlate with a True or False label (1 or -1) \n",
    "    search:  search a specific part of the sentence ('left'|'right'|'between'|'sentence')\n",
    "    window:  the length of tokens to match against for ('left'|'right') search spaces \n",
    "\n",
    "### 2. Term Matching Factory\n",
    "We illustrate below how you can use the `MatchTerms` factory to create and test an LF on training candidates. When examining candidates in the `SentenceNgramViewer`, notice that treatment always occurs between disease and chemical names. That is the supervision signal encoded by this LF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = {'treatment'}\n",
    "\n",
    "# we'll initialize our LFG and test its coverage on training candidates\n",
    "LF_treatment_btw = MatchTerms(name='treatment', terms=terms, label=1, search='between').lf()\n",
    "\n",
    "# what candidates are covered by this LF?\n",
    "labeled = coverage(session, LF_treatment_btw, split=0)\n",
    "\n",
    "# now let's view what this LF labeled\n",
    "SentenceNgramViewer(labeled, session, n_per_page=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viewing Error Buckets\n",
    "If we have gold labeled data, we can evaluate formal metrics. It's useful to view specific errors for a given LF input in the `SentenceNgramViewer`.\n",
    "\n",
    "Below, we'll compute our empirical scores using human-labeled development set data and then look at any false positive matches by our `LF_treatment_btw` LF. We can see below from our scores that this LF isn't very accurate -- only 24% precision!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = error_analysis(session, LF_treatment_btw, split=1, gold=L_gold_dev)\n",
    "\n",
    "# now let's view what this LF labeled\n",
    "SentenceNgramViewer(tp, session, n_per_page=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Regular Expression Factory\n",
    "\n",
    "Sometimes we want to express more generic textual patterns to match against candidates. Perhaps we want to match a specific phrase like '`did not`' or look for modifier suffixes like `[-]*induced` .\n",
    "\n",
    "We can generate this supervision in the same way as above using sets of [regular expressions](https://en.wikipedia.org/wiki/Regular_expression) -- a formal language for string matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negation_rgxs = {'did not'}\n",
    "\n",
    "LF_negation_btw = MatchRegex(name='negation', rgxs=negation_rgxs, label=-1, search='between').lf()\n",
    "labeled = coverage(session, LF_negation_btw, split=1)\n",
    "\n",
    "# now let's view what this LF labeled\n",
    "SentenceNgramViewer(labeled, session, n_per_page=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = error_analysis(session, LF_negation_btw, split=1, gold=L_gold_dev)\n",
    "SentenceNgramViewer(tn, session, n_per_page=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Distant Supervision Labeling Functions\n",
    "\n",
    "We'll use the [Comparative Toxicogenomics Database (CTD)](http://ctdbase.org/) for distant supervision. The CTD lists chemical-condition entity pairs under three categories: \n",
    "- **Therapy**: the chemical treats the condition\n",
    "- **Marker**: the chemical is typically present with the condition\n",
    "- **Unspecified**: ...unspecified\n",
    "\n",
    "We provide 3 helper functions to test if a candidate is in one of the categories. These use `c.get_cids()` to access each entity mention's normalized concept identifier ([MeSH codes](https://en.wikipedia.org/wiki/List_of_MeSH_codes))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "from six.moves.cPickle import load\n",
    "\n",
    "with open('data/ctd.pkl', 'rb') as ctd_f:\n",
    "    ctd_unspecified, ctd_therapy, ctd_marker = load(ctd_f)\n",
    "    \n",
    "def cand_in_ctd_unspecified(c):\n",
    "    return 1 if c.get_cids() in ctd_unspecified else 0\n",
    "\n",
    "def cand_in_ctd_therapy(c):\n",
    "    return 1 if c.get_cids() in ctd_therapy else 0\n",
    "\n",
    "def cand_in_ctd_marker(c):\n",
    "    return 1 if c.get_cids() in ctd_marker else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LF_ctd_marker(c):\n",
    "    return 1 if cand_in_ctd_marker(c) else 0\n",
    "\n",
    "score(session, LF_ctd_marker, split=1, gold=L_gold_dev)\n",
    "\n",
    "SentenceNgramViewer(labeled, session, n_per_page=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Writing Custom Labeling Functions\n",
    "\n",
    "The strength of LFs is that you can write any arbitrary function and use it to supervise a classification task. This approach can combine many of the same strategies discussed above or encode other information. \n",
    "\n",
    "For example, we observe that when mentions of chemical and disease names occur far apart in a sentence, this is a good indicator that the candidate's label is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LF_too_far_apart(c):\n",
    "    \"\"\"Entity mentions occur at a distance > 10 words\"\"\"\n",
    "    return -1 if len(list(get_between_tokens(c))) > 10 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled = coverage(session, LF_too_far_apart, split=1)\n",
    "\n",
    "score(session, LF_too_far_apart, split=1, gold=L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Composing Labeling Functions\n",
    "\n",
    "Another useful technique for writing LFs is composing multiple, weaker LFs together. For example, our `LF_treatment_btw` example above has low precision.  Instead of modifying `LF_treatment_btw`, we'll compose it with our `LF_too_far_apart` from above.\n",
    "\n",
    "    LF_treatment_btw                              Precision: 27.7 | TP: 14 | FP: 45  \n",
    "    LF_treatment_btw AND NOT LF_too_far_apart     Precision: 34.6 | TP: 9  | FP: 17 \n",
    "\n",
    "We dropped 5 true candidates, but we cut our false positive rate by 28 candidates !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LF_treatment_AND_too_far_apart(c):\n",
    "    return 1 if LF_too_far_apart(c) != -1 and LF_treatment_btw(c) == 1 else 0\n",
    "\n",
    "score(session, LF_treatment_AND_too_far_apart, split=1, gold=L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Development Sandbox\n",
    "----\n",
    "\n",
    "## A. Writing Your Own Labeling Functions\n",
    "\n",
    "Using the information above, write your own labeling functions for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# PLACE YOUR LFs HERE\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Applying Labeling Functions\n",
    "---\n",
    "\n",
    "Next, we need to actually run the LFs over **all** of our training candidates, producing a set of `Labels` and `LabelKeys` (just the names of the LFs) in the database.  We'll do this using the `LabelAnnotator` class, a UDF which we will again run with `UDFRunner`.\n",
    "\n",
    "### 1. Preparing your Labeling Functions\n",
    "\n",
    "First we put all our labeling functions into list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LFs = [\n",
    "    # Place your lf function variable names here\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we setup the label annotator class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snorkel.annotations import LabelAnnotator\n",
    "labeler = LabelAnnotator(lfs=LFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generating the Label Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1701)\n",
    "\n",
    "%time L_train = labeler.apply(split=0, parallelism=1)\n",
    "print(L_train.shape)\n",
    "\n",
    "%time L_dev = labeler.apply_existing(split=1, parallelism=1)\n",
    "print(L_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_train.lf_stats(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Label Matrix Empirical Accuracies\n",
    "\n",
    "If we have a small set of human-labeled data, we can also check the empirical accuracies of LFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_dev.lf_stats(session, labels=L_gold_dev.toarray().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## C. Iterating on Labeling Function Design\n",
    "\n",
    "When writing labeling functions, you will want to iterate on the process outlined above several times. You should focus on tuning individual LFs, based on emprical accuracy metrics, and adding new LFs to improve coverage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-butt": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
